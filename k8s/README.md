# Guia de Instala√ß√£o do Kubernetes com Kubeadm - Single Control Plane

Este guia tem como objetivo auxiliar na cria√ß√£o de um cluster Kubernetes utilizando o kubeadm em uma arquitetura Single Control Plane (sem alta disponibilidade). Ser√£o abordados desde os requisitos at√© a configura√ß√£o do ambiente e os passos para inicializa√ß√£o e configura√ß√£o do cluster.

Esta abordagem √© ideal para ambientes de desenvolvimento, aprendizado ou testes, onde a simplicidade √© a prioridade. Por√©m, √© importante entender as limita√ß√µes desse modelo para tomar decis√µes informadas ao migrar para ambientes de produ√ß√£o.

Aqui, vamos utilizar o kubeadm. O kubeadm √© uma ferramenta projetada para simplificar a configura√ß√£o do Kubernetes. Ele fornece comandos padronizados para inicializar e configurar clusters, reduzindo significativamente o esfor√ßo necess√°rio para colocar um cluster em funcionamento. Este guia detalha cada etapa, desde a prepara√ß√£o do ambiente at√© a execu√ß√£o de uma aplica√ß√£o no cluster, fornecendo tamb√©m explica√ß√µes sobre os conceitos fundamentais para garantir um aprendizado s√≥lido.

## Requisitos

Para criar o cluster Kubernetes, voc√™ deve ter os requisitos de hardware e de rede.

### Requisitos de Hardware
*   3 M√°quinas Linux (aqui no caso vou utilizar Ubuntu 24.04 LTS)
*   2 GB de mem√≥ria RAM
*   2 CPUs
*   Conex√£o de rede entre as m√°quinas
*   Hostname, endere√ßo MAC e product_uuid √∫nicos pra cada n√≥.
*   Swap desabilitado
*   Acesso SSH a todas as m√°quinas

### Requisitos de rede

#### Control Plane
O control plane tem como requisito a libera√ß√£o das seguintes portas:

| Protocol | Direction | Port Range | Purpose | Used By |
| :--- | :--- | :--- | :--- | :--- |
| TCP | Inbound | 6443 | Kubernetes API server | All |
| TCP | Inbound | 2379-2380 | etcd server client API | kube-apiserver, etcd |
| TCP | Inbound | 10250 | Kubelet API | Self, Control plane |
| TCP | Inbound | 10259 | kube-scheduler | Self |
| TCP | Inbound | 10257 | kube-controller-manager | Self |

#### Worker Nodes
Os worker nodes tem como requisito a libera√ß√£o das seguintes portas:

| Protocol | Direction | Port Range | Purpose | Used By |
| :--- | :--- | :--- | :--- | :--- |
| TCP | Inbound | 10250 | Kubelet API | Self, Control plane |
| TCP | Inbound | 10256 | kube-proxy | Self, Load balancers |
| TCP | Inbound | 30000-32767 | NodePort Services‚Ä† | All |

## Passos da Instala√ß√£o

Para a instala√ß√£o e cria√ß√£o do cluster Kubernetes, vamos executar as seguintes etapas:

1.  Instala√ß√£o do Container Runtime (ContainerD)
2.  Instala√ß√£o do kubeadm, kubelet e kubectl
3.  Inicializa√ß√£o do cluster Kubernetes
4.  Instala√ß√£o do CNI Calico
5.  Incluir os worker nodes no cluster Kubernetes

### Instala√ß√£o Autom√°tica (Script)

Para facilitar a instala√ß√£o na VPS (Ubuntu 24.04), criamos um script `setup_vps.sh` que automatiza todo o processo de prepara√ß√£o e inicializa√ß√£o do Control Plane.

Execute o script na m√°quina:

```bash
chmod +x setup_vps.sh
sudo ./setup_vps.sh
```

### Instala√ß√£o Manual

#### Instala√ß√£o do Container Runtime
O Container Runtime √© a base para executar cont√™ineres no Kubernetes. O ContainerD √© uma escolha popular devido √† sua simplicidade, desempenho e suporte oficial do Kubernetes.

**OBS: Essa etapa deve ser executada em todas as m√°quinas que v√£o fazer parte do cluster Kubernetes.**

#### Instala√ß√£o dos m√≥dulos de Kernel do Linux
Para seguir com a instala√ß√£o, primeiro √© preciso habilitar 2 m√≥dulos no kernel do Linux:

*   **overlay** ‚áí Usado pra unir camadas de file system.
*   **br_netfilter** ‚áí Modulo de rede usado pra garantir a comunica√ß√£o dos containers e do Kubernetes.

Comandos para habilitar:

```bash
cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter
```

Ajustes de defini√ß√µes do Kernel:

```bash
# Configura√ß√£o dos par√¢metros do sysctl, fica mantido mesmo com reebot da m√°quina.
cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

# Aplica as defini√ß√µes do sysctl sem reiniciar a m√°quina
sudo sysctl --system
```

*   `net.bridge.bridge-nf-call-iptables = 1` ‚áí Ativo as redes bridges pra passarem pelo iptables e assim as regras de firewall v√£o passar por essas redes tamb√©m.
*   `net.ipv4.ip_forward = 1` ‚áí Habilita o encaminhamento de pacotes IPv4 no sistema. Isso √© essencial para que o host funcione como um roteador, encaminhando pacotes de rede de uma interface para outra.
*   `net.bridge.bridge-nf-call-ip6tables = 1` ‚áí Similar ao bridge-nf-call-iptables, mas para tr√°fego IPv6.

#### Instala√ß√£o do ContainerD
OBS: A partir da vers√£o 1.26 do Kubernetes, foi removido o suporte ao CRI v1alpha2 e ao Containerd 1.5. Vamos usar o reposit√≥rio do Docker para instalar uma vers√£o recente.

```bash
# Instala√ß√£o de pr√© requisitos
sudo apt-get update -y
sudo apt-get install ca-certificates curl gnupg --yes
sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
sudo chmod a+r /etc/apt/keyrings/docker.gpg

# Configurando o reposit√≥rio
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update

sudo apt update -y && sudo apt install containerd.io -y
```

Configura√ß√£o padr√£o do Containerd:

```bash
sudo mkdir -p /etc/containerd && containerd config default | sudo tee /etc/containerd/config.toml
```

Alterar o arquivo de configura√ß√£o pra configurar o systemd cgroup driver. Sem isso o Containerd n√£o gerencia corretamente os recursos computacionais e vai reiniciar em loop.

```bash
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml

# Alterar a imagem do sandbox
sudo sed -i 's|sandbox_image = ".*"|sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml
```

Agora √© preciso reiniciar o containerd:

```bash
sudo systemctl restart containerd
```

#### Instala√ß√£o do kubeadm, kubelet and kubectl
Com o containerd instalado, agora √© preciso instalar o kubeadm, kubelet e o kubectl.

```bash
# Instala√ß√£o dos pacotes necess√°rios
sudo apt-get update && \
sudo apt-get install -y apt-transport-https ca-certificates curl

# Download da chave p√∫blica do Reposit√≥rio do Kubernetes
# Nota: Ajustado para v1.31 conforme solicitado, mas verifique a vers√£o desejada.
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# Adicionando o reposit√≥rio apt do Kubernetes
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

# Atualiza√ß√£o do reposit√≥rio apt e instala√ß√£o das ferramentas
sudo apt-get update && \
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
```

#### Iniciando o Cluster Kubernetes
Inicializo o cluster Kubernetes.

**OBS: Esse comando precisa ser executado apenas no control plane**

```bash
# Inicializa√ß√£o do cluster Kubernetes
# Substitua o IP abaixo pelo IP da sua VPS (38.242.232.109)
sudo kubeadm init \
    --apiserver-advertise-address=38.242.232.109 \
    --pod-network-cidr=192.168.0.0/16
```

#### Configura√ß√£o do kubectl
Com o Kubernetes iniciado, agora √© preciso configurar o kubectl para se comunicar com o Api Server:

```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

#### Joins dos Worker Nodes
Para adicionar worker nodes futuramente:

```bash
kubeadm token create --print-join-command
```

#### Instala√ß√£o do CNI
Agora, √© preciso adicionar o Calico para gerenciar a rede dos pods e dos containers:

```bash
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.29.0/manifests/tigera-operator.yaml
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.29.0/manifests/custom-resources.yaml
```

## Verificando a Instala√ß√£o

Ap√≥s a execu√ß√£o do script, verifique se o cluster est√° saud√°vel com os seguintes comandos:

1.  **Verificar os N√≥s**:
    ```bash
    kubectl get nodes
    ```
    *O status deve estar como `Ready` ap√≥s alguns minutos.*

2.  **Verificar os Pods do Sistema e Aplica√ß√£o**:
    ```bash
    kubectl get pods -A
    ```
    *Verifique se os pods `frontend`, `backend`, `postgres` e `traefik` est√£o com status `Running`.*

3.  **Verificar Ingress**:
    ```bash
    kubectl get ingress
    ```
    *Deve listar o `app-ingress` com o endere√ßo IP da VPS.*

## Acessando a Aplica√ß√£o

A aplica√ß√£o deve estar acess√≠vel diretamente pelo IP da sua VPS:

*   **Frontend**: `http://38.242.232.109`
*   **Backend API**: `http://38.242.232.109/api`

## Acessando o Traefik Dashboard

O painel do Traefik roda na porta 8080, mas por seguran√ßa n√£o √© exposto publicamente por padr√£o. Para acessar via navegador:

1.  **Na sua m√°quina local** (onde voc√™ tem o `kubectl` configurado com o arquivo da VPS), rode:
    ```bash
    kubectl port-forward -n traefik $(kubectl get pods -n traefik -l app=traefik -o jsonpath='{.items[0].metadata.name}') 8080:8080
    ```

2.  Acesse no navegador:
    *   **Dashboard**: [http://localhost:8080/dashboard/](http://localhost:8080/dashboard/) (Aten√ß√£o para a barra no final)
    *   **API**: [http://localhost:8080/api](http://localhost:8080/api)

## Acessando o Banco de Dados (PostgreSQL)

O banco de dados n√£o √© exposto publicamente por seguran√ßa. Para acessar via DBeaver ou PgAdmin da sua m√°quina local:

1.  **Crie um t√∫nel seguro**:
    ```bash
    kubectl port-forward svc/postgres 5432:5432
    ```

2.  **Configure o DBeaver**:
    *   **Host**: `localhost`
    *   **Port**: `5432`
    *   **Database**: `payment_api`
    *   **Username**: `postgres`
    *   **Password**: `password`

## Rodando a aplica√ß√£o
Teste o cluster Kubernetes usando o manifesto abaixo:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  selector:
    app: nginx
  ports:
  - port: 80
    targetPort: 80
  type: LoadBalancer
```


## üêô Instala√ß√£o do ArgoCD (Material Complementar)

Abaixo est√£o os comandos e refer√™ncias para a instala√ß√£o do ArgoCD no seu cluster Kubernetes.

### üìÑ Documenta√ß√£o Oficial
- [Manual de Instala√ß√£o](https://argo-cd.readthedocs.io/en/stable/operator-manual/installation/)
- [Instala√ß√£o sem Alta Disponibilidade (Non-HA)](https://argo-cd.readthedocs.io/en/stable/operator-manual/installation/#non-high-availability)
- [Instala√ß√£o com Alta Disponibilidade (HA)](https://argo-cd.readthedocs.io/en/stable/operator-manual/installation/#high-availability)

### üõ†Ô∏è Comandos de Instala√ß√£o (Manifestos)

#### Instala√ß√£o Padr√£o (Non-HA)
Recomendado para testes e demonstra√ß√µes.

```bash
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
```

#### Instala√ß√£o com Alta Disponibilidade (HA)
Recomendado para produ√ß√£o.

```bash
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/ha/install.yaml
```

### üì¶ Instala√ß√£o via Helm

Para instala√ß√£o via Helm, consulte os seguintes recursos:

- **Documenta√ß√£o**: [ArgoCD Helm Installation](https://argo-cd.readthedocs.io/en/stable/operator-manual/installation/#helm)
- **Artifact Hub**: [argo-cd Helm Chart](https://artifacthub.io/packages/helm/argo/argo-cd#installing-the-chart)
- **GitHub**: [argo-helm Repository](https://github.com/argoproj/argo-helm/tree/main/charts/argo-cd#installing-the-chart)